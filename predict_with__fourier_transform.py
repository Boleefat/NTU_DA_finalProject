# -*- coding: utf-8 -*-
"""predict_with_ Fourier_transform

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FBnBJjxgQQNpJh4dAQQeey3e03PMtvry
"""

from google.colab import drive
drive.mount('/content/drive')
import warnings
warnings.filterwarnings("ignore")

# import tensorflow as tf
# print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

# !pip install astral

import pandas as pd

def make_df(csv_file_path):
    """
    Read a CSV file, convert the 'TIMESTAMP' column to datetime, and sort the DataFrame by the timestamp index.

    Args:
        csv_file_path (str): The file path to the CSV file.

    Returns:
        DataFrame: The DataFrame containing the data from the CSV file sorted by timestamp index.
    """
    # Read the CSV file
    df = pd.read_csv(csv_file_path)

    # Convert 'TIMESTAMP' column to datetime and remove timezone information
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP']).dt.strftime('%Y-%m-%d %H:%M:%S')

    # Convert 'TIMESTAMP' column to datetime type
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])

    # Set 'TIMESTAMP' column as the index
    df.set_index('TIMESTAMP', inplace=True)

    # Sort the DataFrame by the timestamp index in ascending order
    df.sort_index(inplace=True)

    return df

def count_values_summary(df):
    """
    Count the number of NaN, blank, 0, and negative values for each column in the DataFrame
    and return the result summary as a table.

    Args:
        df (DataFrame): DataFrame to count the values in.

    Returns:
        DataFrame: A DataFrame containing the counts of NaN, blank, 0, and negative values for each column.
    """
    # Count NaN values for each column
    nan_counts = df.isna().sum()

    # Count blank values for each column
    blank_counts = (df == '').sum()

    # Count 0 values for each column
    zero_counts = (df == 0).sum()

    # Count negative values for each column
    negative_counts = (df < 0).sum()

    # Initialize a DataFrame to store the counts
    value_counts_summary = pd.DataFrame({
        'NaN count': nan_counts,
        'Blank count': blank_counts,
        'Zero count': zero_counts,
        'Negative count': negative_counts
    })

    return value_counts_summary

from sklearn.metrics import mean_squared_error as mse
import matplotlib.pyplot as plt


def plot_predictions1(model, X, y, start=0, end=300):
  predictions = model.predict(X).flatten()
  df = pd.DataFrame(data={'Predictions':predictions, 'Actuals':y})
  plt.plot(df['Predictions'][start:end])
  plt.plot(df['Actuals'][start:end])
  return df, mse(y, predictions)

from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae

def test_mae_rmse(model, X, y, model_name="Model"):
    # Predict
    predictions = model.predict(X).flatten()

    # Calculate RMSE and MAE
    rmse_value = mse(y, predictions, squared=False)
    mae_value = mae(y, predictions)

    # Print the results
    print(f"{model_name} - MAE: {mae_value}")
    print(f"{model_name} - RMSE: {rmse_value}")

    return mae_value, rmse_value

"""# Prepare the data"""

# file_path = '/content/drive/MyDrive/PV/NIST/Output/prepared2016.csv'

# ground_df=make_df(file_path)

# # Resample the data to 10-minute intervals for the specified float columns
# resampled_df = ground_df[['Ambient_Temp', 'GHI', 'POA', 'WindSpeed_Horiz',
#                           'WindDir', 'WindSpeed', 'BacksheetTemp', 'PowerOutput']].resample('10T').mean()

# resampled_df.info()

# resampled_df.head(10)

# import pandas as pd
# from datetime import datetime
# from astral import LocationInfo
# from astral.sun import sun
# import pytz
# from sklearn.preprocessing import LabelEncoder

# def add_time_features(df, lat, long, timezone='UTC'):
#     df.index = pd.to_datetime(df.index)
#     df['DayOfYear'] = df.index.dayofyear
#     df['Month'] = df.index.month
#     df['hour'] = df.index.hour
#     df['minute'] = df.index.minute

#     def get_season(date, latitude, longitude, timezone='UTC'):
#         location = LocationInfo(latitude=latitude, longitude=longitude, timezone=timezone)
#         year = date.year

#         if date.tzinfo is None:
#             date = date.replace(tzinfo=pytz.UTC)

#         spring_equinox = sun(location.observer, date=datetime(year, 3, 20, tzinfo=pytz.UTC))['noon']
#         summer_solstice = sun(location.observer, date=datetime(year, 6, 21, tzinfo=pytz.UTC))['noon']
#         autumn_equinox = sun(location.observer, date=datetime(year, 9, 22, tzinfo=pytz.UTC))['noon']
#         winter_solstice = sun(location.observer, date=datetime(year, 12, 21, tzinfo=pytz.UTC))['noon']

#         if date < spring_equinox:
#             return "Winter"
#         elif spring_equinox <= date < summer_solstice:
#             return "Spring"
#         elif summer_solstice <= date < autumn_equinox:
#             return "Summer"
#         elif autumn_equinox <= date < winter_solstice:
#             return "Autumn"
#         else:
#             return "Winter"

#     df['Season'] = df.index.map(lambda date: get_season(date, lat, long, timezone))
#     label_encoder = LabelEncoder()
#     df['Season'] = label_encoder.fit_transform(df['Season'])

#     # Define is_daytime column
#     df['is_daytime'] = 0  # Initialize with 0
#     df.loc[((df['hour'] >= 6) & (df['hour'] <= 18)), 'is_daytime'] = 1

#     return df

# # Example usage
# # Example location for Gaithersburg, Maryland, USA
# latitude = 39.1434
# longitude = -77.2014
# timezone = 'US/Eastern'

# ground_df = add_time_features(resampled_df, latitude, longitude, timezone)

# # Display the updated dataframe
# print(ground_df.head())

# import numpy as np

# # Cyclical encoding for DayOfYear, Month, hour, and minute
# for col in ['DayOfYear', 'Month', 'hour', 'minute']:
#     ground_df[f'{col}_sin'] = np.sin(2 * np.pi * ground_df[col] / ground_df[col].max())
#     ground_df[f'{col}_cos'] = np.cos(2 * np.pi * ground_df[col] / ground_df[col].max())
#     ground_df.drop(col, axis=1, inplace=True)

# ground_df.info()

# count_values_summary(ground_df)

# from sklearn.preprocessing import MinMaxScaler


# # Replace NaN values with zeros
# ground_df = ground_df.fillna(0)

# count_values_summary(ground_df)

# ground_df.to_csv('/content/drive/MyDrive/PV/NIST/Output/ground_feed16.csv')

"""# Feed the data"""

file_path = '/content/drive/MyDrive/PV/NIST/Output/ground_feed16.csv'

ground_df=make_df(file_path)

ground_df.info()

# Define the start and end dates of the period you want to select
start_date = '2016-01-01'
end_date = '2016-03-31'
# Select the subset of data based on the specified period
selected_data = ground_df.loc[start_date:end_date]

# Ensure the data is sorted by the datetime index
selected_data = selected_data.sort_index()

# Print the first and last few rows to check the order
print("First few rows of selected data:")
print(selected_data.head())

print("\nLast few rows of selected data:")
print(selected_data.tail())

selected_data.info()

columns = list(selected_data.columns)

# Move 'PowerOutput' to the first position
columns.insert(0, columns.pop(columns.index('PowerOutput')))

# Reorder the DataFrame
selected_data = selected_data[columns]

# Print the DataFrame to check the new order of columns
print(selected_data.head())

selected_data.info()

from sklearn.preprocessing import MinMaxScaler

# Separate the target variable
power_output = selected_data['PowerOutput']
features = selected_data.drop(columns=['PowerOutput'])

# Apply MinMaxScaler to the features
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(features)

# Convert the scaled features back to a DataFrame
scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns, index=selected_data.index)

# Combine the target variable and the scaled features
selected_data = pd.concat([power_output, scaled_features_df], axis=1)

print(selected_data.head())

import numpy as np
# X: [[[t1], [t2], [t3], [t4], [t5]]] y: [t6]
# X: [[[t2], [t3], [t4], [t5], [t6]]] y: [t7]
# X: [[[t3], [t4], [t5], [t6], [t7]]] y: [t8]


def df_to_X_y2(df, window_size=144):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [r for r in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size][0]
    y.append(label)
  return np.array(X), np.array(y)

window_size = 144
X2, y2 = df_to_X_y2(selected_data, window_size)
X2.shape, y2.shape

X2_train, y2_train = X2[:7260], y2[:7260]
X2_val, y2_val=X2[7260:9670], y2[7260:9670]
X2_test, y2_test=X2[9670:], y2[9670:]

X2_train.shape, y2_train.shape, X2_val.shape, y2_val.shape, X2_test.shape, y2_test.shape

"""# LSTM"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.layers import InputLayer, Input, Bidirectional, LSTM, Dense
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Set the random seed
np.random.seed(42)
tf.random.set_seed(42)

# model architecture
model_LSTM= Sequential()
model_LSTM.add(InputLayer((144, 18))) #(InputLayer((number of timesteps, number of variabel))
model_LSTM.add(LSTM(64))
model_LSTM.add(Dense(8, 'relu'))
model_LSTM.add(Dense(1, 'linear'))

model_LSTM.summary()

cp1 = ModelCheckpoint('model_LSTM/', save_best_only=True)
model_LSTM.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# model1.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=50, batch_size=64, callbacks=[cp1])

# Fit the model with batch size and save the history
history = model_LSTM.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp1])

# Extract loss and validation loss per epoch
train_loss1 = history.history['loss']
val_loss1 = history.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss1, val_loss1)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss1, label='Train Loss')
plt.plot(val_loss1, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE per epoch
train_rmse1 = history.history['root_mean_squared_error']
val_rmse1 = history.history['val_root_mean_squared_error']

train_mae1 = history.history['mean_absolute_error']
val_mae1 = history.history['val_mean_absolute_error']


# Print the RMSE and MAE per epoch

for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse1, val_rmse1, train_mae1, val_mae1)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

# Plot the training and validation RMSE
plt.plot(train_rmse1, label='Train RMSE')
plt.plot(val_rmse1, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_rmse1, label='Train RMSE')
plt.plot(val_rmse1, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_mae1, label='Train MAE')
plt.plot(val_mae1, label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

# plt.subplot(1, 3, 3)
# plt.plot(train_mape2, label='Train MAPE')
# plt.plot(val_mape2, label='Validation MAPE')
# plt.xlabel('Epoch')
# plt.ylabel('MAPE')
# plt.legend()

plt.tight_layout()
plt.show()

plot_predictions1(model_LSTM, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_LSTM, X2_test, y2_test)

"""# Bidirectional LSTM"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError
from tensorflow.keras.optimizers import Adam

# Set the random seed
np.random.seed(42)
tf.random.set_seed(42)

# Define the model with Bidirectional LSTM
model_BidirLSTM = Sequential()
model_BidirLSTM.add(Input(shape=(144, 18)))
model_BidirLSTM.add(Bidirectional(LSTM(64)))
model_BidirLSTM.add(Dense(8, activation='relu'))
model_BidirLSTM.add(Dense(1, activation='linear'))

model_BidirLSTM.summary()

# Define the model checkpoint callback
cp2 = ModelCheckpoint('model_BidirLSTM/', save_best_only=True)

# Compile the model with additional metrics
model_BidirLSTM.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Fit the model with batch size and save the history
# history2 = model2.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=10, batch_size=32, callbacks=[cp2])

history2 = model_BidirLSTM.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp2], verbose=2)

# Extract loss and validation loss per epoch
train_loss2 = history2.history['loss']
val_loss2 = history2.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss2, val_loss2)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss2, label='Train Loss')
plt.plot(val_loss2, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

train_rmse2 = history2.history['root_mean_squared_error']
val_rmse2 = history2.history['val_root_mean_squared_error']

train_mae2 = history2.history['mean_absolute_error']
val_mae2 = history2.history['val_mean_absolute_error']

# train_mape2 = history2.history['mean_absolute_percentage_error']
# val_mape2 = history2.history['val_mean_absolute_percentage_error']


# for epoch, (train_r, val_r, train_m, val_m, train_p, val_p) in enumerate(zip(train_rmse2, val_rmse2, train_mae2, val_mae2, train_mape2, val_mape2)):
#     print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}, Train MAPE = {train_p}, Validation MAPE = {val_p}")

for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse2, val_rmse2, train_mae2, val_mae2)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_rmse2, label='Train RMSE')
plt.plot(val_rmse2, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_mae2, label='Train MAE')
plt.plot(val_mae2, label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

# plt.subplot(1, 3, 3)
# plt.plot(train_mape2, label='Train MAPE')
# plt.plot(val_mape2, label='Validation MAPE')
# plt.xlabel('Epoch')
# plt.ylabel('MAPE')
# plt.legend()

plt.tight_layout()
plt.show()

# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.metrics import mean_absolute_error, mean_squared_error
# import numpy as np

# # Flatten predictions if necessary (e.g., if they are in shape (n, 1) instead of (n,))
# predictions = predictions.flatten()

# # Compute MAE
# mae = mean_absolute_error(y2_test, predictions)

# # Compute MSE
# mse = mean_squared_error(y2_test, predictions)

# # Compute RMSE
# rmse = np.sqrt(mse)

# # Print the results
# print(f"Test MAE: {mae}")
# print(f"Test MSE: {mse}")
# print(f"Test RMSE: {rmse}")

plot_predictions1(model_BidirLSTM, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_BidirLSTM, X2_test, y2_test)

"""# GRU + Bidirectional"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, GRU, Dense, Bidirectional
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Set the random seed
np.random.seed(42)
tf.random.set_seed(42)

# Define the model with GRU
model_BidirGRU = Sequential()
model_BidirGRU.add(InputLayer((144, 18)))
model_BidirGRU.add(Bidirectional(GRU(64)))
model_BidirGRU.add(Dense(8, activation='relu'))
model_BidirGRU.add(Dense(1, activation='linear'))

model_BidirGRU.summary()

# Define the model checkpoint callback
cp3 = ModelCheckpoint('model_BidirGRU/', save_best_only=True)

# Compile the model with additional metrics
model_BidirGRU.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Fit the model with batch size and save the history
history3 = model_BidirGRU.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp3])

# Extract loss and validation loss per epoch
train_loss3 = history3.history['loss']
val_loss3 = history3.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss3, val_loss3)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss3, label='Train Loss')
plt.plot(val_loss3, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse3 = history3.history['root_mean_squared_error']
val_rmse3 = history3.history['val_root_mean_squared_error']

train_mae3 = history3.history['mean_absolute_error']
val_mae3 = history3.history['val_mean_absolute_error']

# train_mape3 = history3.history['mean_absolute_percentage_error']
# val_mape3 = history3.history['val_mean_absolute_percentage_error']

# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse3, val_rmse3, train_mae3, val_mae3)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

# Plot the training and validation RMSE, MAE, and MAPE
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_rmse3, label='Train RMSE')
plt.plot(val_rmse3, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_mae3, label='Train MAE')
plt.plot(val_mae3, label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

# plt.subplot(1, 3, 3)
# plt.plot(train_mape3, label='Train MAPE')
# plt.plot(val_mape3, label='Validation MAPE')
# plt.xlabel('Epoch')
# plt.ylabel('MAPE')
# plt.legend()

plt.tight_layout()
plt.show()

plot_predictions1(model_BidirGRU, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_BidirGRU, X2_test, y2_test)

"""# GRU"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, GRU, Dense
from tensorflow.keras.layers import Input, GRU, Dense, Activation, Dot, Concatenate, Softmax
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt


# Define the model with GRU
model_GRU = Sequential()
model_GRU.add(InputLayer((144, 18)))
model_GRU.add(GRU(64))  # Using a GRU layer without bidirectional processing
model_GRU.add(Dense(8, activation='relu'))
model_GRU.add(Dense(1, activation='linear'))

model_GRU.summary()
# Define the model checkpoint callback
cp6 = ModelCheckpoint('model_GRU/', save_best_only=True)

# Compile the model with additional metrics
model_GRU.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Fit the model with batch size and save the history
history6 = model_GRU.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp6])

# Extract loss and validation loss per epoch
train_loss6 = history6.history['loss']
val_loss6 = history6.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss6, val_loss6)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss6, label='Train Loss')
plt.plot(val_loss6, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse6 = history6.history['root_mean_squared_error']
val_rmse6 = history6.history['val_root_mean_squared_error']

train_mae6 = history6.history['mean_absolute_error']
val_mae6 = history6.history['val_mean_absolute_error']

# train_mape3 = history3.history['mean_absolute_percentage_error']
# val_mape3 = history3.history['val_mean_absolute_percentage_error']

# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse6, val_rmse6, train_mae6, val_mae6)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

# Plot the training and validation RMSE, MAE, and MAPE
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_rmse6, label='Train RMSE')
plt.plot(val_rmse6, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_mae6, label='Train MAE')
plt.plot(val_mae6, label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

# plt.subplot(1, 3, 3)
# plt.plot(train_mape3, label='Train MAPE')
# plt.plot(val_mape3, label='Validation MAPE')
# plt.xlabel('Epoch')
# plt.ylabel('MAPE')
# plt.legend()

plt.tight_layout()
plt.show()

plot_predictions1(model_GRU, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_GRU, X2_test, y2_test)

"""# VARMAX"""

file_path = '/content/drive/MyDrive/PV/NIST/Output/ground_feed16.csv'

ground_df=make_df(file_path)

# Define the start and end dates of the period you want to select
start_date = '2016-01-01'
end_date = '2016-03-31'
# Select the subset of data based on the specified period
selected_data = ground_df.loc[start_date:end_date]

# Ensure the data is sorted by the datetime index
selected_data = selected_data.sort_index()

# Print the first and last few rows to check the order
print("First few rows of selected data:")
print(selected_data.head())

print("\nLast few rows of selected data:")
print(selected_data.tail())

columns = list(selected_data.columns)

# Move 'PowerOutput' to the first position
columns.insert(0, columns.pop(columns.index('PowerOutput')))

# Reorder the DataFrame
selected_data = selected_data[columns]

# Print the DataFrame to check the new order of columns
print(selected_data.head())

import pandas as pd
from statsmodels.tsa.stattools import adfuller
import numpy as np

# Load your data into the 'selected_data' DataFrame
# Assuming 'selected_data' is already loaded
# Load the prepared data
# data = pd.read_csv('/content/drive/MyDrive/PV/NIST/Output/stationarityVARMAX.csv', index_col=0, parse_dates=True)

columns_to_drop = ['DayOfYear_sin', 'Month_cos', 'Season', 'is_daytime', 'DayOfYear_sin', 'DayOfYear_cos', 'Month_sin', 'Month_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos']
selected_data=selected_data.drop(columns=columns_to_drop)

selected_data.info()

# Step 1: Ensure Stationarity
def test_stationarity(timeseries):
    result = adfuller(timeseries)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    for key, value in result[4].items():
        print('Critical Values:')
        print(f'   {key}, {value}')
    return result[1]  # Return p-value for stationarity check

# Apply the stationarity test to the 'PowerOutput' column
p_value = test_stationarity(selected_data['PowerOutput'])
if p_value > 0.05:
    selected_data['PowerOutput_diff'] = selected_data['PowerOutput'].diff().dropna()
    # Repeat the stationarity test after differencing
    p_value_diff = test_stationarity(selected_data['PowerOutput_diff'].dropna())
    if p_value_diff > 0.05:
        print("Data is still not stationary. Further differencing may be required.")
    else:
        target_column = 'PowerOutput_diff'
else:
    selected_data['PowerOutput_diff'] = selected_data['PowerOutput']
    target_column = 'PowerOutput'
    print("Data is stationary.")

import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Plot the time series to visualize trends and seasonality
plt.figure(figsize=(14, 7))
plt.plot(selected_data.index, selected_data['PowerOutput'], label='PowerOutput')
plt.title('PowerOutput Over Time')
plt.xlabel('Date')
plt.ylabel('PowerOutput')
plt.legend()
plt.show()

# Decompose the time series to check for trends and seasonality
# Note: Adjust the period based on your data frequency
decomposition = seasonal_decompose(selected_data['PowerOutput'], model='additive', period=144)  # Example period
fig = decomposition.plot()
plt.show()

# Create seasonal dummy variables (example: daily seasonality)
selected_data['hour'] = selected_data.index.hour
selected_data['minute'] = selected_data.index.minute
selected_data = pd.get_dummies(selected_data, columns=['hour', 'minute'], drop_first=True)

# Create lagged features
lags = 3  # Number of lags to create
for column in selected_data.columns:
    for lag in range(1, lags + 1):
        selected_data[f'{column}_lag{lag}'] = selected_data[column].shift(lag)

# Drop rows with NaN values created by lagging
selected_data.dropna(inplace=True)

# Prepare the final dataset for VARMAX
features = selected_data.drop(columns=['PowerOutput', 'PowerOutput_diff'], errors='ignore')
target = selected_data[['PowerOutput_diff']]

# Combine the target and features
selected_data_final = pd.concat([target, features], axis=1)

# Save the data for model training in next steps
selected_data_final.to_csv('/content/drive/MyDrive/PV/NIST/Output/stationarityVARMAX.csv')

# Load the prepared data
data_final = pd.read_csv('/content/drive/MyDrive/PV/NIST/Output/stationarityVARMAX.csv', index_col=0, parse_dates=True)
data_final = data_final.asfreq('10T')

# Check for multicollinearity
correlation_matrix = data_final.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title("Correlation Matrix Heatmap")
plt.show()

data_final.info()

from sklearn.decomposition import PCA

# Standardize the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_final)

# Apply PCA to reduce dimensions
pca = PCA(n_components=40)  # Adjust the number of components as needed
data_pca = pca.fit_transform(data_scaled)
data_pca = pd.DataFrame(data_pca, index=data_final.index)

# Split the data into training and test sets
train_size = int(len(data_pca) * 0.8)
train_data = data_pca.iloc[:train_size]
test_data = data_pca.iloc[train_size:]

# Fit a simple VAR model
from statsmodels.tsa.api import VAR

model_var = VAR(train_data)
results_var = model_var.fit(maxlags=3, ic='aic')

# Forecast
lag_order = results_var.k_ar
forecast_input = train_data.values[-lag_order:]
forecast = results_var.forecast(y=forecast_input, steps=len(test_data))
forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=[f'PC{i+1}' for i in range(data_pca.shape[1])])

# Since PCA components are not directly interpretable, evaluate overall performance
mse = mean_squared_error(test_data.iloc[:, 0], forecast_df.iloc[:, 0])
mae = mean_absolute_error(test_data.iloc[:, 0], forecast_df.iloc[:, 0])
rmse = np.sqrt(mse)
r2 = r2_score(test_data.iloc[:, 0], forecast_df.iloc[:, 0])
print(f'PCA-based VAR Model Performance:')
print('Mean Squared Error:', mse)
print('Mean Absolute Error:', mae)
print('Root Mean Squared Error:', rmse)
print('R-squared:', r2)

# Visualize the forecasted results vs actual values for the first principal component
plt.figure(figsize=(14, 7))
plt.plot(test_data.index, test_data.iloc[:, 0], label='Actual (PC1)')
plt.plot(forecast_df.index, forecast_df.iloc[:, 0], label='Forecast (PC1)', linestyle='--')
plt.title('Forecast vs Actual for PCA-based VAR Model (First Principal Component)')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

# Standardize the data
scaler = StandardScaler()
data_final_scaled = scaler.fit_transform(data_final)
data_final_scaled = pd.DataFrame(data_final_scaled, columns=data_final.columns, index=data_final.index)

# Split the data into training and test sets
train_size = int(len(data_final_scaled) * 0.8)
train_data = data_final_scaled.iloc[:train_size]
test_data = data_final_scaled.iloc[train_size:]

# Experiment with different model parameters
orders = [(1, 0), (1, 1), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3)]

for order in orders:
    print(f"Trying VARMAX model with order={order}")
    model = VARMAX(train_data, order=order)

    try:
        # Train the model
        model_fitted = model.fit(disp=False)

        # Forecast future values
        n_forecast = len(test_data)
        pred = model_fitted.get_forecast(steps=n_forecast)
        pred_df = pred.predicted_mean

        # Evaluate the model
        mse = mean_squared_error(test_data, pred_df)
        mae = mean_absolute_error(test_data, pred_df)
        rmse = np.sqrt(mse)
        r2 = r2_score(test_data, pred_df)
        print(f"Model with order={order}:")
        print('Mean Squared Error:', mse)
        print('Mean Absolute Error:', mae)
        print('Root Mean Squared Error:', rmse)
        print('R-squared:', r2)

        # Visualize the forecasted results vs actual values
        plt.figure(figsize=(14, 7))
        plt.plot(test_data.index, test_data.iloc[:, 0], label='Actual')
        plt.plot(pred_df.index, pred_df.iloc[:, 0], label='Forecast', linestyle='--')
        plt.title(f'Forecast vs Actual for VARMAX order={order}')
        plt.xlabel('Date')
        plt.ylabel('Value')
        plt.legend()
        plt.show()

        break  # Exit the loop if a model fits successfully
    except Exception as e:
        print(f"An error occurred with order={order}:")
        print(e)

"""Skip"""

# Step 2: Select Features
# For simplicity, we will use all available features

# Prepare the final dataset for VARMAX
# data = selected_data.dropna()  # Drop rows with NaN values from differencing
features = selected_data.drop(columns=['PowerOutput', 'PowerOutput_diff'], errors='ignore')
target = selected_data[[target_column]]

# Combine the target and features
selected_data_final = pd.concat([target, features], axis=1)

# Display the prepared selected_data
print(selected_data_final.head())

# Save the selected_data for model training in next steps
selected_data_final.to_csv('/content/drive/MyDrive/PV/NIST/Output/stationarityVARMAX.csv')

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.statespace.varmax import VARMAX
from sklearn.metrics import mean_squared_error, mean_absolute_error


# Load the prepared data
data_final = pd.read_csv('/content/drive/MyDrive/PV/NIST/Output/stationarityVARMAX.csv', index_col=0, parse_dates=True)
data_final = data_final.asfreq('10T')

# Check for multicollinearity
correlation_matrix = data_final.corr()


# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title("Correlation Matrix Heatmap")
plt.show()

# Identify highly correlated pairs (after dropping some columns)
threshold = 0.9
highly_correlated_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            colname1 = correlation_matrix.columns[i]
            colname2 = correlation_matrix.columns[j]
            highly_correlated_pairs.append((colname1, colname2, correlation_matrix.iloc[i, j]))

print("Highly correlated pairs (correlation > 0.9):")
for pair in highly_correlated_pairs:
    print(f"{pair[0]} and {pair[1]}: {pair[2]:.2f}")

# Drop highly correlated features from the beginning
columns_to_drop = ['GHI', 'WindSpeed_Horiz']
data_final = data_final.drop(columns=columns_to_drop)

data_final.info()

# Check for multicollinearity
correlation_matrix = data_final.corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title("Correlation Matrix Heatmap")
plt.show()

# Standardize the data
scaler = StandardScaler()
data_final_scaled = scaler.fit_transform(data_final)
data_final_scaled = pd.DataFrame(data_final_scaled, columns=data_final.columns, index=data_final.index)

# Split the data into training and test sets
train_size = int(len(data_final_scaled) * 0.8)
train_data = data_final_scaled.iloc[:train_size]
test_data = data_final_scaled.iloc[train_size:]

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.statespace.varmax import VARMAX
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Experiment with different model parameters
orders = [(1, 0), (1, 1), (2, 1), (2, 2)]

for order in orders:
    print(f"Trying VARMAX model with order={order}")
    model = VARMAX(train_data, order=order)

    try:
        # Train the model
        model_fitted = model.fit(disp=False)

        # Forecast future values
        n_forecast = len(test_data)
        pred = model_fitted.get_forecast(steps=n_forecast)
        pred_df = pred.predicted_mean

        # Evaluate the model
        mse = mean_squared_error(test_data, pred_df)
        mae = mean_absolute_error(test_data, pred_df)
        rmse = np.sqrt(mse)
        r2 = r2_score(test_data, pred_df)
        print(f"Model with order={order}:")
        print('Mean Squared Error:', mse)
        print('Mean Absolute Error:', mae)
        print('Root Mean Squared Error:', rmse)
        print('R-squared:', r2)

        # Visualize the forecasted results vs actual values
        plt.figure(figsize=(14, 7))
        plt.plot(test_data.index, test_data.iloc[:, 0], label='Actual')
        plt.plot(pred_df.index, pred_df.iloc[:, 0], label='Forecast', linestyle='--')
        plt.title(f'Forecast vs Actual for VARMAX order={order}')
        plt.xlabel('Date')
        plt.ylabel('Value')
        plt.legend()
        plt.show()

        break  # Exit the loop if a model fits successfully
    except Exception as e:
        print(f"An error occurred with order={order}:")
        print(e)

"""# LSTM with Attention"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Activation, Dot, Concatenate, Softmax
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Set the random seed
np.random.seed(42)
tf.random.set_seed(42)

# Define Attention Layer
class CustomizedAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(CustomizedAttention, self).__init__()
        self.W1 = Dense(units)
        self.W2 = Dense(units)
        self.V = Dense(1)

    def call(self, features, hidden):
        hidden_with_time_axis = tf.expand_dims(hidden, 1)
        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# Define the LSTM model with Attention
def create_lstm_attention_model(input_shape):
    inputs = Input(shape=input_shape)
    lstm = LSTM(64, return_sequences=True)(inputs)
    context_vector, attention_weights = CustomizedAttention(64)(lstm, lstm)
    output = Dense(1, activation='linear')(context_vector)
    model = Model(inputs=inputs, outputs=output)
    return model

# # Adjust the input shape in the function definition
# def create_lstm_attention_model(input_shape):
#     inputs = Input(shape=input_shape[1:])  # Remove the first dimension (batch size)
#     lstm = LSTM(64, return_sequences=True)(inputs)
#     context_vector, attention_weights = Attention(64)(lstm, lstm)
#     output = Dense(1, activation='linear')(context_vector)
#     model = Model(inputs=inputs, outputs=output)
#     return model

# Create the model
model_LSTMAtt = create_lstm_attention_model((144, 18))
model_LSTMAtt.summary()

# Define the model checkpoint callback
cp4 = ModelCheckpoint('model_LSTMAtt/', save_best_only=True)

# Compile the model with additional metrics
model_LSTMAtt.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Fit the model with batch size and save the history
history4 = model_LSTMAtt.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp4])

# Extract loss and validation loss per epoch
train_loss4 = history4.history['loss']
val_loss4 = history4.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss4, val_loss4)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss4, label='Train Loss')
plt.plot(val_loss4, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse4 = history4.history['root_mean_squared_error']
val_rmse4 = history4.history['val_root_mean_squared_error']

train_mae4 = history4.history['mean_absolute_error']
val_mae4 = history4.history['val_mean_absolute_error']

# train_mape4 = history4.history['mean_absolute_percentage_error']
# val_mape4 = history4.history['val_mean_absolute_percentage_error']

# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse4, val_rmse4, train_mae4, val_mae4)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

# Plot the training and validation RMSE, MAE, and MAPE
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_rmse4, label='Train RMSE')
plt.plot(val_rmse4, label='Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_mae4, label='Train MAE')
plt.plot(val_mae4, label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

# plt.subplot(1, 3, 3)
# plt.plot(train_mape4, label='Train MAPE')
# plt.plot(val_mape4, label='Validation MAPE')
# plt.xlabel('Epoch')
# plt.ylabel('MAPE')
# plt.legend()

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

def plot_predictions2(model, X, y, start=0, end=500):
    # Predict
    predictions = model.predict(X)

    # Extract the final prediction for each sample
    predictions = predictions[:, -1, 0]

    # Ensure the lengths of predictions and actuals are the same
    if len(predictions) != len(y):
        raise ValueError("Lengths of predictions and actuals must be the same")

    # Calculate RMSE
    rmse = mean_squared_error(y, predictions, squared=False)
    print(f"RMSE: {rmse}")

    # Create DataFrame
    df = pd.DataFrame({'Predictions': predictions, 'Actuals': y})

    # Plot predictions and actuals
    plt.plot(df['Predictions'][start:end], label='Predictions')
    plt.plot(df['Actuals'][start:end], label='Actuals')
    plt.legend()
    plt.show()

    return df

plot_predictions2(model_LSTMAtt, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_LSTMAtt, X2_test, y2_test)

"""## LSTM with attention + dropout"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, InputLayer
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError
from tensorflow.keras.optimizers import Adam
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error


# Set the random seed
np.random.seed(42)
tf.random.set_seed(42)

# Define the LSTM model with Attention and Dropout
def create_lstm_attention_model_with_dropout(input_shape, dropout_rate=0.2):
    inputs = Input(shape=input_shape)
    lstm = LSTM(64, return_sequences=True)(inputs)
    lstm = Dropout(dropout_rate)(lstm)  # Add dropout after LSTM
    context_vector, attention_weights = CustomizedAttention(64)(lstm, lstm)
    context_vector = Dropout(dropout_rate)(context_vector)  # Add dropout after attention
    output = Dense(1, activation='linear')(context_vector)
    model = Model(inputs=inputs, outputs=output)
    return model

# Create the model with dropout
model_LSTMDrop = create_lstm_attention_model_with_dropout((144, 18), dropout_rate=0.2)
model_LSTMDrop.summary()

# Define the model checkpoint callback
cp_LSTMDrop = ModelCheckpoint('model_LSTMDrop/', save_best_only=True)

# Compile the model with additional metrics
model_LSTMDrop.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001),
                           metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Train the model
history_LSTMDrop = model_LSTMDrop.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=20, batch_size=32, callbacks=[cp_LSTMDrop])

# Extract loss and validation loss per epoch
train_loss5 = history5.history['loss']
val_loss5 = history5.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss5, val_loss5)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss5, label='Train Loss')
plt.plot(val_loss5, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse5 = history_LSTMDrop.history['root_mean_squared_error']
val_rmse5 = history_LSTMDrop.history['val_root_mean_squared_error']

train_mae5 = history_LSTMDrop.history['mean_absolute_error']
val_mae5 = history_LSTMDrop.history['val_mean_absolute_error']

# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse5, val_rmse5, train_mae5, val_mae5)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

def plot_predictions2(model, X, y, start=0, end=500):
    # Predict
    predictions = model.predict(X)

    # Extract the final prediction for each sample
    predictions = predictions[:, -1, 0]

    # Ensure the lengths of predictions and actuals are the same
    if len(predictions) != len(y):
        raise ValueError("Lengths of predictions and actuals must be the same")

    # Calculate RMSE
    rmse = mean_squared_error(y, predictions, squared=False)
    print(f"RMSE: {rmse}")

    # Create DataFrame
    df = pd.DataFrame({'Predictions': predictions, 'Actuals': y})

    # Plot predictions and actuals
    plt.plot(df['Predictions'][start:end], label='Predictions')
    plt.plot(df['Actuals'][start:end], label='Actuals')
    plt.legend()
    plt.show()

    return df

plot_predictions2(model_LSTMDrop, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(model_LSTMDrop, X2_test, y2_test)

"""## LSTM with attention + dropout + regularization"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Activation, Dot, Concatenate, Softmax, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt

# Set the random seed for the model
np.random.seed(42)
tf.random.set_seed(42)

# Define Attention Layer
class CustomAttention(tf.keras.layers.Layer):
    def __init__(self, units, dropout_rate=0.2):
        super(CustomAttention, self).__init__()
        self.W1 = Dense(units, kernel_regularizer=l2(0.01))  # Add L2 regularization to the kernel weights
        self.W2 = Dense(units, kernel_regularizer=l2(0.01))  # Add L2 regularization to the kernel weights
        self.V = Dense(1)
        self.dropout = Dropout(dropout_rate)

    def call(self, features, hidden):
        hidden_with_time_axis = tf.expand_dims(hidden, 1)
        score = tf.nn.tanh(self.dropout(self.W1(features)) + self.dropout(self.W2(hidden_with_time_axis)))  # Apply dropout
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# Define the LSTM model with Attention and regularization
def create_lstm_attention_model_with_regularization(input_shape, dropout_rate=0.2):
    inputs = Input(shape=input_shape)
    lstm = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)  # Add L2 regularization to the kernel weights
    lstm = Dropout(dropout_rate)(lstm)  # Add dropout
    context_vector, attention_weights = CustomAttention(64, dropout_rate)(lstm, lstm)  # Pass dropout_rate to Attention layer
    output = Dense(1, activation='linear', kernel_regularizer=l2(0.01))(context_vector)  # Add L2 regularization to the kernel weights
    model = Model(inputs=inputs, outputs=output)
    return model

# Create the model with regularization and dropout
model_LSTMAttRegDrop = create_lstm_attention_model_with_regularization((144, 18), dropout_rate=0.2)
model_LSTMAttRegDrop.summary()

# Define the model checkpoint callback
cp_LSTMAttRegDrop = ModelCheckpoint('model_LSTMAttRegDrop/', save_best_only=True)

# Compile the model with additional metrics
model_LSTMAttRegDrop.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Train the model
history_LSTMAttRegDrop = model_LSTMAttRegDrop.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=10, callbacks=[cp_LSTMAttRegDrop])

# Extract loss and validation loss per epoch
train_loss6 = history_LSTMAttRegDrop.history['loss']
val_loss6 = history_LSTMAttRegDrop.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss6, val_loss6)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss6, label='Train Loss')
plt.plot(val_loss6, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse6 = history_LSTMAttRegDrop.history['root_mean_squared_error']
val_rmse6 = history_LSTMAttRegDrop.history['val_root_mean_squared_error']

train_mae6 = history_LSTMAttRegDrop.history['mean_absolute_error']
val_mae6 = history_LSTMAttRegDrop.history['val_mean_absolute_error']


# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse6, val_rmse6, train_mae6, val_mae6)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

def plot_predictions2(model, X, y, start=0, end=500):
    # Predict
    predictions = model.predict(X)

    # Extract the final prediction for each sample
    predictions = predictions[:, -1, 0]

    # Ensure the lengths of predictions and actuals are the same
    if len(predictions) != len(y):
        raise ValueError("Lengths of predictions and actuals must be the same")

    # Calculate RMSE
    rmse = mean_squared_error(y, predictions, squared=False)
    print(f"RMSE: {rmse}")

    # Create DataFrame
    df = pd.DataFrame({'Predictions': predictions, 'Actuals': y})

    # Plot predictions and actuals
    plt.plot(df['Predictions'][start:end], label='Predictions')
    plt.plot(df['Actuals'][start:end], label='Actuals')
    plt.legend()
    plt.show()

    return df

plot_predictions2(LSTMAttRegDrop, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(LSTMAttRegDrop, X2_test, y2_test)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Activation, Dot, Concatenate, Softmax, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt

# Set the random seed for the model
np.random.seed(42)
tf.random.set_seed(42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, GRU, Dense

# Define the model with GRU
modelGRU = Sequential()
modelGRU.add(InputLayer((144, 18)))
modelGRU.add(GRU(64))  # Using a GRU layer without bidirectional processing
modelGRU.add(Dense(8, activation='relu'))
modelGRU.add(Dense(1, activation='linear'))

modelGRU.summary()

# Define the model checkpoint callback
cp_GRU = ModelCheckpoint('modelGRU/', save_best_only=True)

# Compile the model with additional metrics
modelGRU.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError(), MeanAbsoluteError()])

# Train the model
history_modelGRU = modelGRU.fit(X2_train, y2_train, validation_data=(X2_val, y2_val), epochs=10, callbacks=[cp_GRU])

# Extract loss and validation loss per epoch
train_loss7 = history_modelGRU.history['loss']
val_loss7 = history_modelGRU.history['val_loss']

# Print the losses per epoch
for epoch, (train, val) in enumerate(zip(train_loss7, val_loss7)):
    print(f"Epoch {epoch+1}: Train Loss = {train}, Validation Loss = {val}")

# Plot the training and validation loss
plt.plot(train_loss7, label='Train Loss')
plt.plot(val_loss7, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract RMSE, MAE, and MAPE per epoch
train_rmse7 = history_modelGRU.history['root_mean_squared_error']
val_rmse7 = history_modelGRU.history['val_root_mean_squared_error']

train_mae7 = history_modelGRU.history['mean_absolute_error']
val_mae7 = history_modelGRU.history['val_mean_absolute_error']


# Print the RMSE, MAE, and MAPE per epoch
for epoch, (train_r, val_r, train_m, val_m) in enumerate(zip(train_rmse7, val_rmse7, train_mae7, val_mae7)):
    print(f"Epoch {epoch+1}: Train RMSE = {train_r}, Validation RMSE = {val_r}, Train MAE = {train_m}, Validation MAE = {val_m}")

plot_predictions1(modelGRU, X2_test, y2_test)

mae_value, rmse_value= test_mae_rmse(modelGRU, X2_test, y2_test)



"""# First Options (please just ignore this part)"""

# import numpy as np

# # Define input and output steps
# input_steps = 1008   # Number of past time steps to use as input
# output_steps = 144  # Number of future time steps to predict

# # Create sequences, excluding last column (wrong method)
# # X = []
# # y = []
# # for i in range(len(scaled_data) - input_steps - output_steps + 1):
# #     X.append(scaled_data[i:i+input_steps, :-1])  # Input sequences (excluding last column)
# #     y.append(scaled_data[i+input_steps:i+input_steps+output_steps, -1])   # Corresponding output sequence (last column)
# # X = np.array(X)
# # y = np.array(y)

# # Create sequences, excluding last column (correct method)
# X = []
# y = []
# for i in range(len(scaled_data) - input_steps - output_steps + 1):
#     X.append(scaled_data[i:i+input_steps])  # Input sequences (include all columns)
#     y.append(scaled_data[i+input_steps:i+input_steps+output_steps, :8])  # Target sequences (first 8 columns)
# X = np.array(X)
# y = np.array(y)


# # X.append(scaled_data[i:i+input_steps]): Includes all columns in the input sequences.
# # y.append(scaled_data[i+input_steps:i+input_steps+output_steps, :8]): Creates target sequences with the first 8 columns (assuming the first 8 columns are Ambient_Temp, GHI, POA, WindSpeed_Horiz, WindDir, WindSpeed, BacksheetTemp, and PowerOutput).
# # Make sure to adjust :8 to match the exact columns you need in 𝑦. If your target columns are not the first 8 columns, you need to adjust the slicing accordingly.

# # This modification should ensure that 𝑋 includes all 18 columns and 𝑦 includes the specific columns you want to forecast.

# # Split the data into training and testing sets
# # (Assuming you want to use the last portion of the data for testing)


# # split = int(0.8 * len(X))
# # X_train, X_test = X[:split], X[split:]
# # y_train, y_test = y[:split], y[split:]

# # Define the split indices for training, validation, and testing
# train_split_index = int(len(X) * 0.6)  # 60% training
# val_split_index = int(len(X) * 0.8)    # 20% validation, 20% testing

# # Split the data into training, validation, and testing sets
# X_train, y_train = X[:train_split_index], y[:train_split_index]
# X_val, y_val = X[train_split_index:val_split_index], y[train_split_index:val_split_index]
# X_test, y_test = X[val_split_index:], y[val_split_index:]

# # Print shapes of training and testing data
# print("Training data shapes:")
# print("X_train:", X_train.shape)
# print("y_train:", y_train.shape)

# print("\nValidation data shapes:")
# print("X_val:", X_val.shape)
# print("y_val:", y_val.shape)

# print("\nTesting data shapes:")
# print("X_test:", X_test.shape)
# print("y_test:", y_test.shape)

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense
# from tensorflow.keras.layers import Reshape

# # Define the LSTM model architecture
# model = Sequential()

# # # Add LSTM layers
# # model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
# # model.add(LSTM(32))


# # Add LSTM layers
# model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
# model.add(LSTM(32))

# # # Add Dense layers for regression
# # model.add(Dense(32, activation='relu'))
# # model.add(Dense(output_steps))  # Output layer with number of units equal to output_steps


# # Add Dense layers for regression
# model.add(Dense(64, activation='relu'))  # Increased the number of neurons for better capacity
# model.add(Dense(output_steps * 8))  # Output layer with number of units equal to output_steps * 8 (since you have 8 features to predict)
# model.add(Reshape((output_steps, 8)))  # Reshape to (output_steps, 8)

# # Compile the model
# model.compile(optimizer='adam', loss='mse')

# # Print the model summary
# model.summary()

# # Train the model using the training data and validate on the validation data
# history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# # Get the training and validation loss values from the history object
# train_loss = history.history['loss']
# val_loss = history.history['val_loss']

# # Print the training and validation loss values
# print("Training Loss:", train_loss)
# print("Validation Loss:", val_loss)

# # Evaluate the model on the testing data
# test_loss = model.evaluate(X_test, y_test)
# print("Test Loss:", test_loss)

# import matplotlib.pyplot as plt

# # Create a plot
# plt.figure(figsize=(10, 6))
# plt.plot(train_loss, label='Training Loss')
# plt.plot(val_loss, label='Validation Loss')
# # plt.axhline(y=test_loss, color='r', linestyle='--', label=f'Test Loss: {test_loss:.4f}')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.title('Training, Validation, and Testing Loss')
# plt.legend()
# plt.show()

# # Assuming you have trained your model and have input data X_test
# # Generate predictions for the test data
# predictions = model.predict(X_test)

# # Print the shape of the predictions array
# print("Shape of predictions array:", predictions.shape)

# # Optionally, you can print some sample predictions
# print("Sample predictions:")
# print(predictions[:5])  # Print the first 5 predictions

# predictions_reshaped = predictions.reshape(-1, 1)  # Flatten the predictions to fit the scaler's expected input shape
# # Create a placeholder array for inverse transformation
# dummy_input = np.zeros((predictions_reshaped.shape[0], selected_data.shape[1]))
# dummy_input[:, -1] = predictions_reshaped[:, 0]  # Assuming the predictions are for the last column (which they are not in this case, hence this might need modification based on exact scaling)

# # Inverse transform
# inverse_transformed_preds = scaler.inverse_transform(dummy_input)[:, -1]

# # Reshape back to the original prediction shape
# inverse_transformed_preds = inverse_transformed_preds.reshape(predictions.shape)

# # Extract the timestamps for the test period
# test_start_index = len(selected_data) - len(X_test) - output_steps + 1
# test_timestamps = selected_data.index[test_start_index:]

# # Create a list to store the DataFrame for each output step
# predicted_dfs = []

# for i in range(output_steps):
#     future_timestamps = test_timestamps[i:i + len(X_test)]
#     predicted_df = pd.DataFrame({
#         'Timestamp': future_timestamps,
#         'Predicted_Value': inverse_transformed_preds[:, i]
#     })
#     predicted_dfs.append(predicted_df)

# # Concatenate all DataFrames to get a single DataFrame with all predictions
# all_predictions_df = pd.concat(predicted_dfs, ignore_index=True)

# all_predictions_df.head()

"""# Another option (do ignore this part as well)"""

# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, RepeatVector

# # Define input and output steps
# input_steps = 1008  # Number of past time steps to use as input
# output_steps = 144  # Number of future time steps to predict

# # Define the selected output features indices (assuming the order in the DataFrame)
# output_features_indices = [0, 1, 2, 3, 4, 5, 6, 7]  # Indices of the 8 features to predict


# # Create sequences
# X = []
# y = []
# for i in range(len(scaled_data) - input_steps - output_steps + 1):
#     X.append(scaled_data[i:i+input_steps, :])  # Input sequences (all 18 features)
#     y.append(scaled_data[i+input_steps:i+input_steps+output_steps, output_features_indices])  # Selected output features (8 features)
# X = np.array(X)
# y = np.array(y)

# # Reshape y to the correct shape (samples, output_steps, number of features to predict)
# y = y.reshape(y.shape[0], output_steps, len(output_features_indices))

# # Define the split indices for training, validation, and testing
# train_split_index = int(len(X) * 0.6)  # 60% training
# val_split_index = int(len(X) * 0.8)    # 20% validation, 20% testing

# # Split the data into training, validation, and testing sets
# X_train, y_train = X[:train_split_index], y[:train_split_index]
# X_val, y_val = X[train_split_index:val_split_index], y[train_split_index:val_split_index]
# X_test, y_test = X[val_split_index:], y[val_split_index:]

# # Define the LSTM model architecture
# model = Sequential()

# # Add LSTM layers
# model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
# model.add(LSTM(32, return_sequences=False))

# # Add a RepeatVector to match the output_steps
# model.add(RepeatVector(output_steps))

# # Add LSTM layer to process the repeated vector
# model.add(LSTM(32, return_sequences=True))

# # Add TimeDistributed layer to match the output shape
# model.add(TimeDistributed(Dense(len(output_features_indices), activation='linear')))


# # Compile the model
# model.compile(optimizer='adam', loss='mse')

# # Print the model summary
# model.summary()

# # Train the model using the training data and validate on the validation data
# history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))

# # Get the training and validation loss values from the history object
# train_loss = history.history['loss']
# val_loss = history.history['val_loss']

# # Print the training and validation loss values
# print("Training Loss:", train_loss)
# print("Validation Loss:", val_loss)

# import matplotlib.pyplot as plt

# # Create a plot
# plt.figure(figsize=(10, 6))
# plt.plot(train_loss, label='Training Loss')
# plt.plot(val_loss, label='Validation Loss')
# # plt.axhline(y=test_loss, color='r', linestyle='--', label=f'Test Loss: {test_loss:.4f}')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.title('Training, Validation, and Testing Loss')
# plt.legend()
# plt.show()

# # Generate predictions for the test data
# predictions = model.predict(X_test)

# # Reshape predictions for DataFrame
# reshaped_predictions = predictions.reshape(-1, len(output_features_indices))

# # Inverse transform the predictions back to the original scale
# # Note: We need to scale the predictions only for the output features
# # Create a temporary array to hold the scaled features
# temp_array = np.zeros((reshaped_predictions.shape[0], scaled_data.shape[1]))
# temp_array[:, output_features_indices] = reshaped_predictions

# # Inverse transform
# inverse_scaled_predictions = scaler.inverse_transform(temp_array)[:, output_features_indices]

# # Generate corresponding timestamps for the predictions
# start_pred_time = selected_data.index[input_steps]
# end_pred_time = selected_data.index[len(selected_data) - output_steps]
# pred_timestamps = pd.date_range(start=start_pred_time, periods=len(predictions) * output_steps, freq='10T')

# # Create DataFrame with predictions and timestamps
# predicted_df = pd.DataFrame(inverse_scaled_predictions, index=pred_timestamps, columns=selected_data.columns[output_features_indices])

# # Print the predicted DataFrame
# predicted_df.shape

# predicted_df.tail()

# # Generate rolling forecasts
# def rolling_forecast(model, initial_input, num_forecasts, input_steps, output_steps):
#     forecasted_data = []
#     current_input = initial_input

#     for _ in range(num_forecasts):
#         # Make predictions
#         prediction = model.predict(current_input)

#         # Store the predictions
#         forecasted_data.append(prediction[0])

#         # Prepare the next input by combining the original input and new prediction
#         new_input = current_input[0, -output_steps:, :]
#         next_input = np.zeros((1, input_steps, new_input.shape[1]))
#         next_input[0, :input_steps-output_steps, :] = current_input[0, output_steps:, :]
#         next_input[0, input_steps-output_steps:, :] = new_input
#         next_input[:, -output_steps:, output_features_indices] = prediction

#         current_input = next_input

#     return np.concatenate(forecasted_data, axis=0)

# # Define the starting point for the forecast
# initial_input = scaled_data[-input_steps:].reshape(1, input_steps, -1)

# # Calculate the number of rolling forecasts required
# num_forecasts = (len(selected_data) - input_steps) // output_steps

# # Generate rolling forecasts
# rolling_predictions = rolling_forecast(model, initial_input, num_forecasts, input_steps, output_steps)

# # Inverse transform the predictions back to the original scale
# # Create a temporary array to hold the scaled features
# temp_array = np.zeros((rolling_predictions.shape[0], scaled_data.shape[1]))
# temp_array[:, output_features_indices] = rolling_predictions

# # Inverse transform
# inverse_scaled_predictions = scaler.inverse_transform(temp_array)[:, output_features_indices]

# # Generate corresponding timestamps for the predictions
# start_pred_time = selected_data.index[input_steps]
# pred_timestamps = pd.date_range(start=start_pred_time, periods=len(inverse_scaled_predictions), freq='10T')

# # Create DataFrame with predictions and timestamps
# predicted_df = pd.DataFrame(inverse_scaled_predictions, index=pred_timestamps, columns=selected_data.columns[output_features_indices])

# # Print the predicted DataFrame
# print(predicted_df.shape)
# print(predicted_df.info())

# predicted_df.tail()

# # Plot the power output over time
# plt.figure(figsize=(14, 7))
# plt.plot(predicted_df.index, predicted_df['PowerOutput'], label='Predicted Power Output')
# plt.xlabel('Date')
# plt.ylabel('Power Output')
# plt.title('Predicted Power Output Over Time')
# plt.legend()
# plt.show()